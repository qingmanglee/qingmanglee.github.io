<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[58沈剑的《架构师之路》学习笔记]]></title>
    <url>%2F2017%2F11%2F05%2F58%E6%B2%88%E5%89%91%E7%9A%84%E3%80%8A%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[前言前端时间关注了58沈剑大牛的“架构师之路”的微信公众号，之前一直没时间认真拜读，现打算利用周末的时间学习一下，做一些笔记。本篇内容全部来自《架构师之路》，我只是在此摘抄记录一些知识点。感谢沈剑大牛的分享。 高可用架构高可用HA（High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系统不能提供服务的时间。 方法论上，高可用是通过冗余+自动故障转移来实现的。整个互联网分层系统架构的高可用，又是通过每一层的冗余+自动故障转移来综合实现的，具体的： 【客户端层】到【反向代理层】的高可用，是通过反向代理层的冗余实现的，常见实践是keepalived + virtual IP自动故障转移 【反向代理层】到【站点层】的高可用，是通过站点层的冗余实现的，常见实践是nginx与web-server之间的存活性探测与自动故障转移 【站点层】到【服务层】的高可用，是通过服务层的冗余实现的，常见实践是通过service-connection-pool来保证自动故障转移 【服务层】到【缓存层】的高可用，是通过缓存数据的冗余实现的，常见实践是缓存客户端双读双写，或者利用缓存集群的主从数据同步与sentinel保活与自动故障转移；更多的业务场景，对缓存没有高可用要求，可以使用缓存服务化来对调用方屏蔽底层复杂性 【服务层】到【数据库“读”】的高可用，是通过读库的冗余实现的，常见实践是通过db-connection-pool来保证自动故障转移 【服务层】到【数据库“写”】的高可用，是通过写库的冗余实现的，常见实践是keepalived + virtual IP自动故障转移 负载均衡负载均衡（Load Balance）是分布式系统架构设计中必须考虑的因素之一，它通常是指，将请求/数据【均匀】分摊到多个操作单元上执行，负载均衡的关键在于【均匀】。 【客户端层】到【反向代理层】的负载均衡，是通过“DNS轮询”实现的 【反向代理层】到【站点层】的负载均衡，是通过“nginx”实现的 【站点层】到【服务层】的负载均衡，是通过“服务连接池”实现的 【数据层】的负载均衡，要考虑“数据的均衡”与“请求的均衡”两个点，常见的方式有“按照范围水平切分”与“hash水平切分” 数据库 数据库使用规范 参见公众号里的文章《58到家数据库30条军规解读》 分库 hash取模：user_id%2=0为0库，user_id%2=1为1库。 数据分段：user_id属于[0, 1亿]为0库，属于[1亿, 2亿]为2库。 引入像mycat等数据库中间件 保证数据的安全性是DBA第一要务，需要进行： 全量备份+增量备份，并定期进行恢复演练，但该方案恢复时间较久，对系统可用性影响大 1小时延时从，双份1小时延时从能极大加速数据库恢复时间 个人建议1小时延时从足够，后台只读服务可以连1小时延时从，提高资源利用率 线程数究竟设多少合理 结论：N核服务器，通过执行业务的单线程分析出本地计算时间为x，等待时间为y，则工作线程数（线程池线程数）设置为 N*(x+y)/x，能让CPU的利用率最大化。 经验：一般来说，非CPU密集型的业务（加解密、压缩解压缩、搜索排序等业务是CPU密集型的业务），瓶颈都在后端数据库，本地CPU计算的时间很少，所以设置几十或者几百个工作线程也都是可能的。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx的rewrite配置]]></title>
    <url>%2F2017%2F11%2F03%2Fnginx%E7%9A%84rewrite%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[参考： https://segmentfault.com/a/1190000008102599 http://seanlook.com/2015/05/17/nginx-location-rewrite/ 背景之前用yii2在框架层即可通过配置urlManager完成url的重写，最近一个纯php项目需要接入集团的单点登录，记录下nginx的rewrite的相关知识。 rewrite规则执行顺序： server块的rewrite指令 location匹配 匹配的location里的rewrite 如果其中某步URI被重写，则重新循环执行1-3，直到找到真实存在的文件；循环超过10次，则返回500 Internal Server Error错误。 location匹配 =开头表示精确匹配 ^~ 开头表示uri以某个常规字符串开头，不是正则匹配 ~ 开头表示区分大小写的正则匹配; ~* 开头表示不区分大小写的正则匹配 / 通用匹配,任何请求都会匹配到 rewrite的flag标志位 last 停止处理当前的rewrite的指令集，并开始搜索与更改后的URI相匹配的location; break 停止处理当前的rewite指令集 redirect 返回302临时重定向。 permanent 返回301永久重定向 小例子1234567891011121314151617181920212223242526272829303132333435server &#123; listen 9033; server_name xxx.com; root /home/admin/project; location / &#123; try_files $uri $uri/ /index.php$is_args$args; autoindex on; index index.php index.html index.htm; &#125; location ~ \.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; location ~ /\.(ht|svn|git) &#123; deny all; &#125; location ^~ /static/images &#123; rewrite ^/static(.*)$ /dev/$1 last; &#125; location ^~ /mis &#123; root /home/admin/project; fastcgi_pass 127.0.0.1:9001; fastcgi_index index.php; include fcgi.conf; rewrite ^/mis(/[^\?]*)?((\?.*)?)$ /mis/index.php$1$2 break; &#125;&#125;]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>rewrite</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper伪分布式集群搭建]]></title>
    <url>%2F2017%2F10%2F26%2Fzookeeper%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[前言ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，参见官网。典型应用场景，如目录服务、配置管理、同步、集群节点选举、消息队列、通知系统等。本文演示搭建一个单机三个实例的伪分布集群。 准备下载并解压zookeeper。创建三个目录，分别为这三个实例的目录环境，并在三个目录中创建内容分别为1，2，3的名称为myid文件。 1234567$tree zoo1/ zoo2/ zoo3zoo1/└── myidzoo2/└── myidzoo3└── myid 配置在zookeeper的配置目录下，分别创建三个名为zoo1.cfg、zoo2.cfg、zoo3.cfg的配置文件。以server.1=10.101.80.35:9110:9210为例，1为对应实例myid内容编号，10.101.80.35为ip，9110为Leader选举的端口，9210为zookeeper实例之间通信的端口，因搭建的伪分布式集群，所以端口各不相同，真实的分布式集群，三个实例的端口通常相同。 内容如下： 1234567891011121314151617181920212223242526272829303132zoo1.cfgtickTime=2000initLimit=10syncLimit=5dataDir=/home/admin/zookeeper/zoo1clientPort=2181server.1=10.101.80.35:9110:9210server.2=10.101.80.35:9111:9211server.3=10.101.80.35:9112:9212zoo2.cfgtickTime=2000initLimit=10syncLimit=5dataDir=/home/admin/zookeeper/zoo2clientPort=2182server.1=10.101.80.35:9110:9210server.2=10.101.80.35:9111:9211server.3=10.101.80.35:9112:9212zoo3.cfgtickTime=2000initLimit=10syncLimit=5dataDir=/home/admin/zookeeper/zoo3clientPort=2183server.1=10.101.80.35:9110:9210server.2=10.101.80.35:9111:9211server.3=10.101.80.35:9112:9212 启动1234567891011cd /home/admin/zookeeper/zookeeper-3.4.10./bin/zkServer.sh start ./conf/zoo1.cfg./bin/zkServer.sh start ./conf/zoo2.cfg./bin/zkServer.sh start ./conf/zoo3.cfg#查看状态$./bin/zkServer.sh status ./conf/zoo1.cfgZooKeeper JMX enabled by defaultUsing config: ./conf/zoo1.cfgMode: follower 命令行操作1234567891011121314151617181920212223242526272829303132333435363738394041#进入集群./bin/zkCli.sh -server 10.101.80.35:2182#创建节点[zk: 10.101.80.35:2182(CONNECTED) 3] create /mynode testCreated /mynode[zk: 10.101.80.35:2182(CONNECTED) 4] ls /[mynode, zookeeper]#查看所有节点[zk: 10.101.80.35:2182(CONNECTED) 7] ls /[mynode, zookeeper]#查看指定节点[zk: 10.101.80.35:2182(CONNECTED) 5] get /mynodetestcZxid = 0x100000004ctime = Thu Oct 26 20:38:29 CST 2017mZxid = 0x100000004mtime = Thu Oct 26 20:38:29 CST 2017pZxid = 0x100000004cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 4numChildren = 0#更新节点内容[zk: 10.101.80.35:2182(CONNECTED) 8] set /mynode hahacZxid = 0x100000004ctime = Thu Oct 26 20:38:29 CST 2017mZxid = 0x100000005mtime = Thu Oct 26 20:43:40 CST 2017pZxid = 0x100000004cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 4numChildren = 0]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yii2 php多进程 出现 MySQL server has gone away]]></title>
    <url>%2F2017%2F08%2F09%2Fyii2-php%E5%A4%9A%E8%BF%9B%E7%A8%8B-%E5%87%BA%E7%8E%B0-MySQL-server-has-gone-away%2F</url>
    <content type="text"><![CDATA[问题&amp;场景在yii2的command，利用多进程消费数据库中的数据时，总是出现gone away。 分析在mysql的官方文档中其实有专门的章节说明这个问题，原因也是各种各样，详见；其中有一条刚好符合我的场景，大意就是，当fork的子进程都共用相同的mysql连接的时候，会出现该错误，每个子进程单独一个mysql连接即可解决。 You can also encounter this error with applications that fork child processes, all of which try to use the same connection to the MySQL server. This can be avoided by using a separate connection for each child process. 解决在子进程执行之前，先把mysql的连接close即可。 代码123456789101112131415for ($i = 0; $i &lt; $processNum; $i++) &#123; $pid = pcntl_fork(); if(!$pid)&#123; // 子进程处理 Yii::$app-&gt;db-&gt;close();// solve 子进程 MySQL server has gone away $this-&gt;_work($tasks[$i]); exit(0); &#125;&#125;Yii::$app-&gt;db-&gt;close();// solve 主进程 MySQL server has gone away$status = null;while(pcntl_waitpid(0, $status, WUNTRACED) != -1)&#123; pcntl_wexitstatus($status); CommonLog::saveLog('子进程结束'."\n");&#125;]]></content>
      <categories>
        <category>php</category>
      </categories>
      <tags>
        <tag>php</tag>
        <tag>yii2</tag>
        <tag>多进程</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于我]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[大学&emsp;&emsp;花名轻芒，是一个崇尚简单、喜爱编程、追求极致的人。大学时期利用业余时间主要在学习lnmp相关技术，开发过几个网站，拿过几个证书，也从此踏上了程序员之路 工作&emsp;&emsp;目前就职于高德地图，各种打杂 技能&emsp;&emsp;对各种编程语言、各种系统都有兴趣，玩过PHP、Golang、Python等，略懂前端 联系&emsp;&emsp;欢迎指正博客中存在的各种问题，邮箱：1060687053@qq.com]]></content>
  </entry>
  <entry>
    <title><![CDATA[标签]]></title>
    <url>%2Ftags%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[分类]]></title>
    <url>%2Fcategories%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
